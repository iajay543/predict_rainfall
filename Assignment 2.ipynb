{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 Assessment 2\n",
    "#### Student Name: Ajay Ganapathy\n",
    "#### Student ID: 29822270\n",
    "\n",
    "Date: 18/10/2019\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This assignment focuses on Machine learning using pyspark. The main goal of this assignment is to predict rainfall tomorrow in Australia using several supervised machine learning algorithms such as Logistic Regression, Gradient Boosting, Decision Trees and Random Forest. This is basically a classification problem where the target/response/dependent variable is categorical i.e. it has 2 classes/categories (Yes/No). In order to do so, we perform various activities which are listed below:\n",
    "\n",
    "* Importing pyspark and Initializing Spark\n",
    "* Data Wrangling such as deleting not-so-important columns, detecting and imputing missing values, etc.\n",
    "* Data Transformation such as converting columns which hold categorical data to numerical values for the machine learning algorithms to learn.\n",
    "* Creating a feature vector and dividing the data into training and test set \n",
    "* Applying machine learning algorithms on the training set, predicting on the test set, reporting the accuracy and error on the test set\n",
    "* Model evaluation using metrics such as Confusion Matrix.\n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create entry points to spark\n",
    "from pyspark import SparkContext, SparkConf # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import isnan, when, count, col, avg, lit\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A Spark Initialization and Loading of Data\n",
    "\n",
    "### Step 01 Importing Spark Session and Initializing Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start, we import SparkContext from pyspark which is the main entry point of Spark Core functionality. It tells the Spark how to access a cluster. We also import a SparkConf object that contains various information about the Spark application such as the number of processors where we specify 4 cores and the name of the application as 'Rain Forecasting'. Further, we import SparkSession from pyspark.sql which is used to create Dataframes and SQL objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reusing an existing spark context instead of creating one. If there is no existing spark context,\n",
    "#then create a spark context with 4 local cores in the machine\n",
    "sc = SparkContext.getOrCreate()\n",
    "if (sc is None):\n",
    "    sc = SparkContext(master=\"local[4]\", appName=\"Rain Forecasting\")\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 02 Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load the dataset, which is a comma seperated value file into a Spark dataframe using the created SparkSession object 'spark' and read.csv() function. Further, to check the types of data stored in our dataframe, we use printSchema() function. We also print the total number of records in our dataframe using df.count() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
      "|               Date|Location|MinTemp|MaxTemp|Rainfall|Evaporation|Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|RainToday|RainTomorrow|\n",
      "+-------------------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
      "|2008-12-01 00:00:00|  Albury|   13.4|   22.9|     0.6|         NA|      NA|          W|           44|         W|       WNW|          20|          24|         71|         22|     1007.7|     1007.1|       8|      NA|   16.9|   21.8|       No|          No|\n",
      "|2008-12-02 00:00:00|  Albury|    7.4|   25.1|       0|         NA|      NA|        WNW|           44|       NNW|       WSW|           4|          22|         44|         25|     1010.6|     1007.8|      NA|      NA|   17.2|   24.3|       No|          No|\n",
      "|2008-12-03 00:00:00|  Albury|   12.9|   25.7|       0|         NA|      NA|        WSW|           46|         W|       WSW|          19|          26|         38|         30|     1007.6|     1008.7|      NA|       2|     21|   23.2|       No|          No|\n",
      "+-------------------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading the dataset\n",
    "weather_df = spark.read.csv('weatherAUS.csv', header=True, inferSchema=True)\n",
    "weather_df.show(3) #displaying top 3 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- Evaporation: string (nullable = true)\n",
      " |-- Sunshine: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- Cloud9am: string (nullable = true)\n",
      " |-- Cloud3pm: string (nullable = true)\n",
      " |-- Temp9am: string (nullable = true)\n",
      " |-- Temp3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printing the dataset schema\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries in the Weather dataset : 142193\n"
     ]
    }
   ],
   "source": [
    "#displaying the total number of records in the dataset\n",
    "print(\"Total number of entries in the Weather dataset :\",weather_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B Data Wrangling\n",
    "\n",
    "### Step 03 Deleting Less Informative Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we delete columns that are not contributing much to the task of predicting rainfall. \n",
    "\n",
    "Location does not have much importance in predicting whether it will rain tomorrow in Australia. \n",
    "Date is more like an index about the information so it does not contribute as well.\n",
    "\n",
    "Further, we even delete columns such as Evaporation, Sunshine, Cloud9am, Cloud3pm, Temp9am, Temp3pm. The columns are dropped using df.drop() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|RainToday|RainTomorrow|\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|   13.4|   22.9|     0.6|          W|           44|         W|       WNW|          20|          24|         71|         22|     1007.7|     1007.1|       No|          No|\n",
      "|    7.4|   25.1|       0|        WNW|           44|       NNW|       WSW|           4|          22|         44|         25|     1010.6|     1007.8|       No|          No|\n",
      "|   12.9|   25.7|       0|        WSW|           46|         W|       WSW|          19|          26|         38|         30|     1007.6|     1008.7|       No|          No|\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping less important columns\n",
    "weather_df=weather_df.drop('Date','Location','Evaporation','Sunshine','Cloud9am','Cloud3pm','Temp9am','Temp3pm')\n",
    "weather_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: string (nullable = true)\n",
      " |-- MaxTemp: string (nullable = true)\n",
      " |-- Rainfall: string (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: string (nullable = true)\n",
      " |-- WindSpeed3pm: string (nullable = true)\n",
      " |-- Humidity9am: string (nullable = true)\n",
      " |-- Humidity3pm: string (nullable = true)\n",
      " |-- Pressure9am: string (nullable = true)\n",
      " |-- Pressure3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking the schema of the dataset to see if the columns have been dropped\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 04 Displaying the count of missing values in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we simply create a list comprehension where we iterate over each column to find the number of missing values denoted by 'NA'. We display their counts using count() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|RainToday|RainTomorrow|\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|    637|    322|    1406|       9330|         9270|     10013|      3778|        1348|        2630|       1774|       3610|      14014|      13981|     1406|           0|\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#list comprehension to count missing values in each column\n",
    "#tabular format representation\n",
    "weather_df_missing_freq = weather_df.select([f.count(f.when(f.col(i).contains('NA'), i)).alias(i) for i in weather_df.columns])\n",
    "\n",
    "weather_df_missing_freq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 05 Missing value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start, we replace all the 'NA' values with Null so that it will help us simplify the imputation process further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|RainToday|RainTomorrow|\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "|   13.4|   22.9|     0.6|          W|           44|         W|       WNW|          20|          24|         71|         22|     1007.7|     1007.1|       No|          No|\n",
      "|    7.4|   25.1|       0|        WNW|           44|       NNW|       WSW|           4|          22|         44|         25|     1010.6|     1007.8|       No|          No|\n",
      "|   12.9|   25.7|       0|        WSW|           46|         W|       WSW|          19|          26|         38|         30|     1007.6|     1008.7|       No|          No|\n",
      "|    9.2|     28|       0|         NE|           24|        SE|         E|          11|           9|         45|         16|     1017.6|     1012.8|       No|          No|\n",
      "|   17.5|   32.3|       1|          W|           41|       ENE|        NW|           7|          20|         82|         33|     1010.8|       1006|       No|          No|\n",
      "|   14.6|   29.7|     0.2|        WNW|           56|         W|         W|          19|          24|         55|         23|     1009.2|     1005.4|       No|          No|\n",
      "|   14.3|     25|       0|          W|           50|        SW|         W|          20|          24|         49|         19|     1009.6|     1008.2|       No|          No|\n",
      "|    7.7|   26.7|       0|          W|           35|       SSE|         W|           6|          17|         48|         19|     1013.4|     1010.1|       No|          No|\n",
      "|    9.7|   31.9|       0|        NNW|           80|        SE|        NW|           7|          28|         42|          9|     1008.9|     1003.6|       No|         Yes|\n",
      "|   13.1|   30.1|     1.4|          W|           28|         S|       SSE|          15|          11|         58|         27|       1007|     1005.7|      Yes|          No|\n",
      "|   13.4|   30.4|       0|          N|           30|       SSE|       ESE|          17|           6|         48|         22|     1011.8|     1008.7|       No|         Yes|\n",
      "|   15.9|   21.7|     2.2|        NNE|           31|        NE|       ENE|          15|          13|         89|         91|     1010.5|     1004.2|      Yes|         Yes|\n",
      "|   15.9|   18.6|    15.6|          W|           61|       NNW|       NNW|          28|          28|         76|         93|      994.3|        993|      Yes|         Yes|\n",
      "|   12.6|     21|     3.6|         SW|           44|         W|       SSW|          24|          20|         65|         43|     1001.2|     1001.8|      Yes|          No|\n",
      "|    9.8|   27.7|    null|        WNW|           50|      null|       WNW|        null|          22|         50|         28|     1013.4|     1010.3|     null|          No|\n",
      "|   14.1|   20.9|       0|        ENE|           22|       SSW|         E|          11|           9|         69|         82|     1012.2|     1010.4|       No|         Yes|\n",
      "|   13.5|   22.9|    16.8|          W|           63|         N|       WNW|           6|          20|         80|         65|     1005.8|     1002.2|      Yes|         Yes|\n",
      "|   11.2|   22.5|    10.6|        SSE|           43|       WSW|        SW|          24|          17|         47|         32|     1009.4|     1009.7|      Yes|          No|\n",
      "|    9.8|   25.6|       0|        SSE|           26|        SE|       NNW|          17|           6|         45|         26|     1019.2|     1017.1|       No|          No|\n",
      "|   11.5|   29.3|       0|          S|           24|        SE|        SE|           9|           9|         56|         28|     1019.3|     1014.8|       No|          No|\n",
      "|   17.1|     33|       0|         NE|           43|        NE|         N|          17|          22|         38|         28|     1013.6|     1008.1|       No|          No|\n",
      "|   20.5|   31.8|       0|        WNW|           41|         W|         W|          19|          20|         54|         24|     1007.8|     1005.7|       No|          No|\n",
      "|   15.3|   30.9|       0|          N|           33|       ESE|        NW|           6|          13|         55|         23|       1011|     1008.2|       No|          No|\n",
      "|   12.6|   32.4|       0|          W|           43|         E|         W|           4|          19|         49|         17|     1012.9|     1010.1|       No|          No|\n",
      "|   16.2|   33.9|       0|        WSW|           35|        SE|       WSW|           9|          13|         45|         19|     1010.9|     1007.6|       No|          No|\n",
      "|   16.9|     33|       0|        WSW|           57|      null|         W|           0|          26|         41|         28|     1006.8|     1003.6|       No|          No|\n",
      "|   20.1|   32.7|       0|        WNW|           48|         N|       WNW|          13|          30|         56|         15|     1005.2|     1001.7|       No|          No|\n",
      "|   19.7|   27.2|       0|        WNW|           46|        NW|       WSW|          19|          30|         49|         22|     1004.8|     1004.2|       No|         Yes|\n",
      "|   12.5|   24.2|     1.2|        WNW|           50|       WSW|        SW|          11|          22|         78|         70|     1005.6|     1003.4|      Yes|          No|\n",
      "|     12|   24.4|     0.8|          W|           39|       WNW|       WNW|          17|          17|         48|         28|     1006.1|     1005.1|       No|          No|\n",
      "|   11.3|   26.5|       0|        WNW|           56|         W|       WNW|          19|          31|         46|         26|     1004.5|     1003.2|       No|          No|\n",
      "|    9.6|   23.9|       0|          W|           41|       WSW|       SSW|          19|          11|         44|         22|     1014.4|     1013.1|       No|          No|\n",
      "|   10.5|   28.8|       0|        SSE|           26|       SSE|         E|          11|           7|         43|         22|     1018.7|     1014.8|       No|          No|\n",
      "|   12.3|   34.6|       0|        WNW|           37|       SSE|        NW|           6|          17|         41|         12|     1015.1|     1010.3|       No|          No|\n",
      "|   12.9|   35.8|       0|        WNW|           41|       ENE|        NW|           6|          26|         41|          9|     1012.6|     1009.2|       No|          No|\n",
      "|   13.7|   37.9|       0|          W|           52|        SE|       WNW|           4|          26|         33|          8|     1010.9|     1006.7|       No|          No|\n",
      "|   16.1|   38.9|       0|          W|           57|         E|         W|           6|          30|         34|         12|       1007|     1002.7|       No|          No|\n",
      "|     14|   28.3|       0|          W|           48|         W|       WSW|          17|          24|         43|         15|     1011.9|     1010.9|       No|          No|\n",
      "|   12.5|   28.4|       0|         NE|           37|       SSE|         S|          20|           9|         38|         16|     1017.8|     1013.7|       No|          No|\n",
      "|     17|   30.8|       0|         NE|           37|       NNE|         E|          15|          11|         36|         24|     1013.4|     1008.1|       No|          No|\n",
      "|   16.9|     32|       0|          S|           31|       SSE|         N|          13|          17|         52|         31|     1009.9|     1006.8|       No|          No|\n",
      "|   17.3|   34.7|       0|         SW|           35|        SE|       WSW|           7|          15|         48|         16|     1014.1|     1012.1|       No|          No|\n",
      "|   17.2|   37.7|       0|        NNW|           35|        SE|        NW|           7|          17|         51|         19|     1015.7|     1010.9|       No|          No|\n",
      "|   17.4|     43|       0|         NW|           39|       SSE|       SSW|           7|          17|         40|          8|     1011.6|     1006.9|       No|          No|\n",
      "|   19.8|   32.7|       0|        WNW|           44|         W|         W|          20|          28|         34|         28|     1008.4|     1009.2|       No|          No|\n",
      "|   14.9|   26.7|       0|         SW|           56|       WSW|        SW|          20|          31|         46|         20|     1014.1|     1012.7|       No|          No|\n",
      "|   10.5|   28.4|       0|         SE|           33|        SE|        SW|          19|          11|         35|         16|     1019.7|     1017.4|       No|          No|\n",
      "|   11.3|   32.2|       0|        WNW|           28|       ENE|       SSW|          17|          15|         34|         17|     1019.7|     1016.2|       No|          No|\n",
      "|   13.9|   36.6|       0|        WNW|           39|       SSE|       NNE|           2|          15|         39|         10|     1015.8|     1010.6|       No|          No|\n",
      "|   18.6|   39.9|       0|        NNW|           61|       SSE|       WNW|           9|          20|         36|         21|     1010.1|     1004.8|       No|          No|\n",
      "+-------+-------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+---------+------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#replacing all the NA values by None (identified as null)\n",
    "weather_df=weather_df.replace('NA',None)\n",
    "weather_df.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visually analysing the top few rows, we can see that all the 'NA' values in the dataframe has been successfully replaced by null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fill the missing data with average value for numeric columns and maximum frequency value for non-numeric columns.\n",
    "\n",
    "But, all the columns are bydefault in String format. So, first we identify which columns takes on numerical values and cast them to numeric types such as Double.\n",
    "\n",
    "By visual analysis of the data, we can see that columns such as MinTemp, MaxTemp, Rainfall, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am and Pressure3pm hold numerical values. So, we cast/convert those columns to Double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#casting the columns that hold numerical values to Double\n",
    "weather_df=weather_df.withColumn(\"MinTemp\", weather_df[\"MinTemp\"].cast(DoubleType())) \\\n",
    "       .withColumn(\"MaxTemp\", weather_df[\"MaxTemp\"].cast(DoubleType())) \\\n",
    "       .withColumn(\"Rainfall\", weather_df[\"Rainfall\"].cast(DoubleType())) \\\n",
    "       .withColumn(\"WindGustSpeed\", weather_df[\"WindGustSpeed\"].cast(DoubleType())) \\\n",
    "       .withColumn(\"WindSpeed9am\", weather_df[\"WindSpeed9am\"].cast(DoubleType())) \\\n",
    "       .withColumn(\"WindSpeed3pm\", weather_df[\"WindSpeed3pm\"].cast(DoubleType())) \\\n",
    "       .withColumn(\"Humidity9am\", weather_df[\"Humidity9am\"].cast(DoubleType())) \\\n",
    "       .withColumn(\"Humidity3pm\", weather_df[\"Humidity3pm\"].cast(DoubleType())) \\\n",
    "       .withColumn(\"Pressure9am\", weather_df[\"Pressure9am\"].cast(DoubleType())) \\\n",
    "       .withColumn(\"Pressure3pm\", weather_df[\"Pressure3pm\"].cast(DoubleType())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinTemp: double (nullable = true)\n",
      " |-- MaxTemp: double (nullable = true)\n",
      " |-- Rainfall: double (nullable = true)\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindGustSpeed: double (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- WindSpeed9am: double (nullable = true)\n",
      " |-- WindSpeed3pm: double (nullable = true)\n",
      " |-- Humidity9am: double (nullable = true)\n",
      " |-- Humidity3pm: double (nullable = true)\n",
      " |-- Pressure9am: double (nullable = true)\n",
      " |-- Pressure3pm: double (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, we can see that the columns that were listed above are converted to Double."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, those columns are converted to numeric type such as Double, we start with the imputation process. We drop the null values from the dataframe using dropna() function and store the updated data in a new dataframe weather_df_no_na . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping all the nulls\n",
    "weather_df_no_na=weather_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112925"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df_no_na.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values still exist in the older dataframe weather_df which we need to impute. So, we select those numeric columns which has the missing values and take the average value of those column observations existing in the new dataframe weather_df_no_na. We further use that value to impute the missing values in the old dataframe weather_df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustSpeed|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+\n",
      "|   13.4|   22.9|     0.6|         44.0|        20.0|        24.0|       71.0|       22.0|     1007.7|     1007.1|\n",
      "|    7.4|   25.1|     0.0|         44.0|         4.0|        22.0|       44.0|       25.0|     1010.6|     1007.8|\n",
      "|   12.9|   25.7|     0.0|         46.0|        19.0|        26.0|       38.0|       30.0|     1007.6|     1008.7|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#selecting numerical columns and creating a dataframe\n",
    "impute_na_df1=weather_df.select('MinTemp' \\\n",
    "        ,'MaxTemp','Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm' \\\n",
    "        ,'Humidity9am','Humidity3pm','Pressure9am','Pressure3pm')\n",
    "impute_na_df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinTemp 12.66\n",
      "MaxTemp 23.66\n",
      "Rainfall 2.38\n",
      "WindGustSpeed 40.79\n",
      "WindSpeed9am 15.18\n",
      "WindSpeed3pm 19.5\n",
      "Humidity9am 67.4\n",
      "Humidity3pm 50.67\n",
      "Pressure9am 1017.43\n",
      "Pressure3pm 1015.05\n"
     ]
    }
   ],
   "source": [
    "#missing value imputation method for numerical columns also known as Mean Imputation\n",
    "for x in impute_na_df1.columns:\n",
    "    meanValue=round(weather_df_no_na.agg(avg(x)).first()[0],2)\n",
    "    print(x,meanValue)\n",
    "    impute_na_df1=impute_na_df1.na.fill(meanValue,(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustSpeed|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+\n",
      "|   13.4|   22.9|     0.6|         44.0|        20.0|        24.0|       71.0|       22.0|     1007.7|     1007.1|\n",
      "|    7.4|   25.1|     0.0|         44.0|         4.0|        22.0|       44.0|       25.0|     1010.6|     1007.8|\n",
      "|   12.9|   25.7|     0.0|         46.0|        19.0|        26.0|       38.0|       30.0|     1007.6|     1008.7|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "impute_na_df1.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we drop the numeric columns with missing values from the old dataframe weather_df and add the same numeric columns from the imputed dataframe impute_na_df1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping the columns with NA values in older dataframe weather_df\n",
    "weather_df=weather_df.drop('MinTemp','MaxTemp','Rainfall','Evaporation','Sunshine','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','Cloud9am','Cloud3pm','Temp9am','Temp3pm')\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the imputed dataframe impute_na_df1 with the older dataframe weather_df on the basis of common column to maintain the latest updated values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating common column in both the dataframes for the merging process\n",
    "df1=weather_df.withColumn('row_index', f.monotonically_increasing_id())\n",
    "df2=impute_na_df1.withColumn('row_index', f.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+---------+------------+---------+\n",
      "|WindGustDir|WindDir9am|WindDir3pm|RainToday|RainTomorrow|row_index|\n",
      "+-----------+----------+----------+---------+------------+---------+\n",
      "|          W|         W|       WNW|       No|          No|        0|\n",
      "|        WNW|       NNW|       WSW|       No|          No|        1|\n",
      "|        WSW|         W|       WSW|       No|          No|        2|\n",
      "+-----------+----------+----------+---------+------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+---------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustSpeed|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|row_index|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+---------+\n",
      "|   13.4|   22.9|     0.6|         44.0|        20.0|        24.0|       71.0|       22.0|     1007.7|     1007.1|        0|\n",
      "|    7.4|   25.1|     0.0|         44.0|         4.0|        22.0|       44.0|       25.0|     1010.6|     1007.8|        1|\n",
      "|   12.9|   25.7|     0.0|         46.0|        19.0|        26.0|       38.0|       30.0|     1007.6|     1008.7|        2|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- WindGustDir: string (nullable = true)\n",
      " |-- WindDir9am: string (nullable = true)\n",
      " |-- WindDir3pm: string (nullable = true)\n",
      " |-- RainToday: string (nullable = true)\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+---------+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+\n",
      "|WindGustDir|WindDir9am|WindDir3pm|RainToday|RainTomorrow|MinTemp|MaxTemp|Rainfall|WindGustSpeed|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|\n",
      "+-----------+----------+----------+---------+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+\n",
      "|          W|         W|       WNW|       No|          No|   13.4|   22.9|     0.6|         44.0|        20.0|        24.0|       71.0|       22.0|     1007.7|     1007.1|\n",
      "|        WNW|       NNW|       WSW|       No|          No|    7.4|   25.1|     0.0|         44.0|         4.0|        22.0|       44.0|       25.0|     1010.6|     1007.8|\n",
      "|        WSW|         W|       WSW|       No|          No|   12.9|   25.7|     0.0|         46.0|        19.0|        26.0|       38.0|       30.0|     1007.6|     1008.7|\n",
      "+-----------+----------+----------+---------+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#merging the imputed dataframe with older dataframe on common generated column ID\n",
    "weather_df=df1.join(df2, df1.row_index==df2.row_index).drop('row_index')\n",
    "weather_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+---------+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+\n",
      "|WindGustDir|WindDir9am|WindDir3pm|RainToday|RainTomorrow|MinTemp|MaxTemp|Rainfall|WindGustSpeed|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|\n",
      "+-----------+----------+----------+---------+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+\n",
      "|       9330|     10013|      3778|     1406|           0|      0|      0|       0|            0|           0|           0|          0|          0|          0|          0|\n",
      "+-----------+----------+----------+---------+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking if the missing values have been properly imputed for numeric columns\n",
    "weather_df.select([count(when(col(c).isNull(), c)).alias(c) for c in weather_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, we can confirm that the missing values have been properly imputed for the numeric columns. We follow the exact same approach for imputing missing values in non-numeric columns. The only key thing is we replace the missing values in a non-numeric column with the most frequent value in that particular column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by selecting the non-numeric columns containing missing values and storing it in a dataframe impute_na_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+---------+\n",
      "|WindGustDir|WindDir9am|WindDir3pm|RainToday|\n",
      "+-----------+----------+----------+---------+\n",
      "|          W|         W|       WNW|       No|\n",
      "|        WNW|       NNW|       WSW|       No|\n",
      "|        WSW|         W|       WSW|       No|\n",
      "+-----------+----------+----------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#selecting non-numerical columns and creating a dataframe\n",
    "impute_na_df2=weather_df.select('WindGustDir','WindDir9am','WindDir3pm','RainToday')\n",
    "impute_na_df2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we perform groupby() operation on each of those columns iteratively to get the frequency of observations. We sort the frequency values in descending order and take the top most value as the maximum frequency of occurence for that observation. Thus, we take that observation to impute the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WindGustDir W\n",
      "WindDir9am N\n",
      "WindDir3pm SE\n",
      "RainToday No\n"
     ]
    }
   ],
   "source": [
    "#missing value imputation method for non-numerical column - Mode Imputation\n",
    "for x in impute_na_df2.columns:\n",
    "    most_freq_value=impute_na_df2.groupBy(x).count().sort('count', ascending=False).first()[0]\n",
    "    print(x, most_freq_value)\n",
    "    impute_na_df2=impute_na_df2.na.fill(most_freq_value, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RainTomorrow: string (nullable = true)\n",
      " |-- MinTemp: double (nullable = false)\n",
      " |-- MaxTemp: double (nullable = false)\n",
      " |-- Rainfall: double (nullable = false)\n",
      " |-- WindGustSpeed: double (nullable = false)\n",
      " |-- WindSpeed9am: double (nullable = false)\n",
      " |-- WindSpeed3pm: double (nullable = false)\n",
      " |-- Humidity9am: double (nullable = false)\n",
      " |-- Humidity3pm: double (nullable = false)\n",
      " |-- Pressure9am: double (nullable = false)\n",
      " |-- Pressure3pm: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping the columns from the old dataframe that has missing values\n",
    "weather_df=weather_df.drop('WindGustDir','WindDir9am','WindDir3pm','RainToday')\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+\n",
      "|RainTomorrow|MinTemp|MaxTemp|Rainfall|WindGustSpeed|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|WindGustDir|WindDir9am|WindDir3pm|RainToday|\n",
      "+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+\n",
      "|          No|   20.1|   32.7|     0.0|         48.0|        13.0|        30.0|       56.0|       15.0|     1005.2|     1001.7|        WNW|         N|       WNW|       No|\n",
      "|          No|   12.0|   24.4|     0.8|         39.0|        17.0|        17.0|       48.0|       28.0|     1006.1|     1005.1|          W|       WNW|       WNW|       No|\n",
      "|         Yes|   17.2|   28.7|     0.0|         39.0|         4.0|        13.0|       79.0|       41.0|     1016.4|     1011.8|          W|       ENE|         N|       No|\n",
      "+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating common column in both the dataframes for the merging process.\n",
    "df1=weather_df.withColumn('row_index', f.monotonically_increasing_id())\n",
    "df2=impute_na_df2.withColumn('row_index', f.monotonically_increasing_id())\n",
    "\n",
    "weather_df= df1.join(df2, df1.row_index==df2.row_index).drop(\"row_index\")\n",
    "weather_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+\n",
      "|RainTomorrow|MinTemp|MaxTemp|Rainfall|WindGustSpeed|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|WindGustDir|WindDir9am|WindDir3pm|RainToday|\n",
      "+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+\n",
      "|           0|      0|      0|       0|            0|           0|           0|          0|          0|          0|          0|          0|         0|         0|        0|\n",
      "+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking if the missing values have been imputed for non-numeric columns\n",
    "weather_df.select([count(when(col(c).isNull(), c)).alias(c) for c in weather_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that all non-numeric columns with missing values has been imputed successfully. Thus, the imputation is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 06 Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we transform the non-numeric columns to numerical columns so that the machine learning algorithm can process it. We do this transformation by using StringIndexer() method to convert them to numbers. In the StringIndexer() method, we specify the input columns to transform and the output columns where we get the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+---------------+--------------+--------------+-------------+-----+\n",
      "|RainTomorrow|MinTemp|MaxTemp|Rainfall|WindGustSpeed|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|WindGustDir|WindDir9am|WindDir3pm|RainToday|WindGustDir-Ind|WindDir9am-Ind|WindDir3pm-Ind|RainToday-Ind|label|\n",
      "+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+---------------+--------------+--------------+-------------+-----+\n",
      "|          No|   20.1|   32.7|     0.0|         48.0|        13.0|        30.0|       56.0|       15.0|     1005.2|     1001.7|        WNW|         N|       WNW|       No|            9.0|           0.0|           7.0|          0.0|  0.0|\n",
      "|          No|   12.0|   24.4|     0.8|         39.0|        17.0|        17.0|       48.0|       28.0|     1006.1|     1005.1|          W|       WNW|       WNW|       No|            0.0|          14.0|           7.0|          0.0|  0.0|\n",
      "|         Yes|   17.2|   28.7|     0.0|         39.0|         4.0|        13.0|       79.0|       41.0|     1016.4|     1011.8|          W|       ENE|         N|       No|            0.0|          10.0|           6.0|          0.0|  1.0|\n",
      "+------------+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+-----------+----------+----------+---------+---------------+--------------+--------------+-------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#encoding non-numerical columns to columns that hold numerical values\n",
    "weather_df=StringIndexer(inputCol='WindGustDir',outputCol='WindGustDir-Ind').fit(weather_df).transform(weather_df)\n",
    "weather_df=StringIndexer(inputCol='WindDir9am',outputCol='WindDir9am-Ind').fit(weather_df).transform(weather_df)\n",
    "weather_df=StringIndexer(inputCol='WindDir3pm',outputCol='WindDir3pm-Ind').fit(weather_df).transform(weather_df)\n",
    "weather_df=StringIndexer(inputCol='RainToday',outputCol='RainToday-Ind').fit(weather_df).transform(weather_df)\n",
    "weather_df=StringIndexer(inputCol='RainTomorrow',outputCol='label').fit(weather_df).transform(weather_df)\n",
    "\n",
    "\n",
    "weather_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+---------------+--------------+--------------+-------------+-----+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustSpeed|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|WindGustDir-Ind|WindDir9am-Ind|WindDir3pm-Ind|RainToday-Ind|label|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+---------------+--------------+--------------+-------------+-----+\n",
      "|   20.1|   32.7|     0.0|         48.0|        13.0|        30.0|       56.0|       15.0|     1005.2|     1001.7|            9.0|           0.0|           7.0|          0.0|  0.0|\n",
      "|   12.0|   24.4|     0.8|         39.0|        17.0|        17.0|       48.0|       28.0|     1006.1|     1005.1|            0.0|          14.0|           7.0|          0.0|  0.0|\n",
      "|   17.2|   28.7|     0.0|         39.0|         4.0|        13.0|       79.0|       41.0|     1016.4|     1011.8|            0.0|          10.0|           6.0|          0.0|  1.0|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+---------------+--------------+--------------+-------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping original columns\n",
    "weather_df=weather_df.drop('WindGustDir','WindDir9am','WindDir3pm','RainToday','RainTomorrow')\n",
    "weather_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see that the non-numeric columns have been properly encoded with numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 07 Building the feature vector and dividing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final stage of data preparation is to consolidate all of the predictor columns into a single column i.e. to build the feature vector.\n",
    "We do this by using VectorAssembler() method as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+---------------+--------------+--------------+-------------+-----+--------------------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustSpeed|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|WindGustDir-Ind|WindDir9am-Ind|WindDir3pm-Ind|RainToday-Ind|label|            features|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+---------------+--------------+--------------+-------------+-----+--------------------+\n",
      "|   20.1|   32.7|     0.0|         48.0|        13.0|        30.0|       56.0|       15.0|     1005.2|     1001.7|            9.0|           0.0|           7.0|          0.0|  0.0|[20.1,32.7,0.0,48...|\n",
      "|   12.0|   24.4|     0.8|         39.0|        17.0|        17.0|       48.0|       28.0|     1006.1|     1005.1|            0.0|          14.0|           7.0|          0.0|  0.0|[12.0,24.4,0.8,39...|\n",
      "|   17.2|   28.7|     0.0|         39.0|         4.0|        13.0|       79.0|       41.0|     1016.4|     1011.8|            0.0|          10.0|           6.0|          0.0|  1.0|[17.2,28.7,0.0,39...|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+---------------+--------------+--------------+-------------+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#bringing all the columns together to create a feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"MinTemp\", \"MaxTemp\", \"Rainfall\",'WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','WindGustDir-Ind','WindDir9am-Ind','WindDir3pm-Ind','RainToday-Ind'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "weather_df = assembler.transform(weather_df)\n",
    "weather_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we divide the dataset into training and test set in the ratio of 70:30 using randomSplit() function. We also set a seed value for reproducable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+---------------+--------------+--------------+-------------+-----+--------------------+\n",
      "|MinTemp|MaxTemp|Rainfall|WindGustSpeed|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|WindGustDir-Ind|WindDir9am-Ind|WindDir3pm-Ind|RainToday-Ind|label|            features|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+---------------+--------------+--------------+-------------+-----+--------------------+\n",
      "|   -4.9|    3.3|     0.2|         26.0|         6.0|        19.5|       64.0|      50.67|    1017.43|    1015.05|            8.0|           3.0|           0.0|          0.0|  0.0|[-4.9,3.3,0.2,26....|\n",
      "|   -4.9|   12.2|     0.0|         44.0|         4.0|        24.0|       67.4|       36.0|     1026.6|     1024.7|            5.0|           9.0|           2.0|          0.0|  0.0|[-4.9,12.2,0.0,44...|\n",
      "|   -2.7|   19.8|     0.0|         43.0|         2.0|        22.0|       80.0|       30.0|     1026.5|     1021.7|           14.0|           4.0|           8.0|          0.0|  0.0|[-2.7,19.8,0.0,43...|\n",
      "+-------+-------+--------+-------------+------------+------------+-----------+-----------+-----------+-----------+---------------+--------------+--------------+-------------+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 0  # set seed for reproducibility\n",
    "\n",
    "#dividing the data into training and test set in 70:30 ratio\n",
    "\n",
    "trainDF, testDF = weather_df.randomSplit([0.7,0.3],seed)\n",
    "trainDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data:  99594\n",
      "Number of test data:  42599\n"
     ]
    }
   ],
   "source": [
    "#counting the number of observations in training and test set\n",
    "print(\"Number of training data: \", trainDF.count())\n",
    "print(\"Number of test data: \", testDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C Apply Machine Learning Algorithms\n",
    "\n",
    "### Step 8 Model Building and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we build machine learning models such as Decision Tree Classifier, Random Forest Classifier, Logistic Regression Classifier and Gradient Boosting Classifier to predict the possibility of rainfall tomorrow. \n",
    "We fit each classifier one by one and evaluate the model using metrics such as Confusion Matrix which determines the number of missclassifications. \n",
    "We also evaluate the model accuracy and error rate on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Decision Trees <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees and their ensembles are popular methods for machine learning tasks of classification and regression which are widely used to interpret and handle categorical variables for a multiclass problem. The spark.ml implementation supports decision trees for binary and multiclass classification and for regression, using both continuous and categorical features. The implementation partitions data by rows, allowing distributed training with millions or even billions of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a Decision Tree classifier where we specify the feature, label columns and the maximum depth of the tree. Then, we call .fit() function for the model to learn from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating the decision tree classifier\n",
    "tree = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "\n",
    "#fitting the model on the training set\n",
    "tree=tree.fit(trainDF) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we perform predictions on the test data and compare to known values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on the test set\n",
    "prediction_dt=tree.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.84932043444714...|\n",
      "|  0.0|       0.0|[0.84932043444714...|\n",
      "|  0.0|       0.0|[0.84932043444714...|\n",
      "|  0.0|       0.0|[0.84932043444714...|\n",
      "|  0.0|       0.0|[0.84932043444714...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#comparing the predictions with known values\n",
    "prediction_dt.select('label','prediction','probability').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we report the accuracy and test error on the test set using a method named MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.83\n",
      "Test Error = 0.17\n"
     ]
    }
   ],
   "source": [
    "# Evaluator method to get metrics such as accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "accuracy_dt = round(evaluator.evaluate(prediction_dt),2)\n",
    "print(\"Accuracy = \",accuracy_dt)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Logistic Regression <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a Machine Learning algorithm which is used for the classification problems, it is a predictive analysis algorithm and based on the concept of probability. The hypothesis of logistic regression tends it to limit the cost function between 0 and 1. Logistic Regression measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic/sigmoid function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the same steps as we did to build Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the logistic regression classifier\n",
    "logistic = LogisticRegression(featuresCol = 'features', labelCol = 'label',    maxIter=10)\n",
    "\n",
    "#fitting the model on the training set\n",
    "logistic = logistic.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on the test set\n",
    "prediction_log = logistic.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.88247113326762...|\n",
      "|  0.0|       0.0|[0.76122721635752...|\n",
      "|  0.0|       0.0|[0.80457114701391...|\n",
      "|  0.0|       0.0|[0.66702475060071...|\n",
      "|  0.0|       0.0|[0.89197265999358...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#comparing the predictions with known  values\n",
    "prediction_log.select('label','prediction','probability').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.82\n",
      "Test Error = 0.18\n"
     ]
    }
   ],
   "source": [
    "# Evaluator method to get metrics such as accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_logit = round(evaluator.evaluate(prediction_log),2)\n",
    "print(\"Accuracy = \",accuracy_logit)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy_logit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Random Forest <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree. In the Random Forests algorithm, each new data point goes through the same process, but now it visits all the different trees in the ensemble, which are grown using random samples of both training data and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeClassificationModel (uid=dtc_e9b7b50396b9) of depth 5 with 37 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_8a32f37bfccd) of depth 5 with 35 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_bdb7673c9a6e) of depth 5 with 35 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_15023e601fa3) of depth 5 with 29 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_0cf30b23fa5b) of depth 5 with 45 nodes]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the random forest classifier\n",
    "\n",
    "forest = RandomForestClassifier(numTrees=5, labelCol=\"label\",\\\n",
    "featuresCol=\"features\")\n",
    "\n",
    "#fitting the model on the training set\n",
    "forest = forest.fit(trainDF)\n",
    "forest.trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on the test set\n",
    "prediction_rf = forest.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.88555505199138...|\n",
      "|  0.0|       0.0|[0.84801451395751...|\n",
      "|  0.0|       0.0|[0.85821416188567...|\n",
      "|  0.0|       0.0|[0.89205198937991...|\n",
      "|  0.0|       0.0|[0.89205198937991...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#comparing the predictions with known values\n",
    "prediction_rf.select('label','prediction','probability').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(14, {0: 0.0058, 1: 0.0058, 2: 0.1134, 3: 0.0552, 4: 0.0058, 5: 0.0003, 6: 0.0055, 7: 0.7134, 8: 0.0112, 9: 0.0366, 10: 0.0065, 11: 0.0032, 12: 0.0019, 13: 0.0356})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.83\n",
      "Test Error = 0.17\n"
     ]
    }
   ],
   "source": [
    "#evaluator method to get metrics such as accuracy\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_rf = round(evaluator.evaluate(prediction_rf),2)\n",
    "print(\"Accuracy = \",accuracy_rf)\n",
    "print(\"Test Error = %g\" % (1 - accuracy_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Gradient Boosting Tree <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. The objective of any supervised learning algorithm is to define a loss function and minimize it. By using gradient descent and updating our predictions based on a learning rate, we can find the values where Mean Squared Error is minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the gradient boosting classifier\n",
    "\n",
    "gbt = GBTClassifier(maxIter=10)\n",
    "\n",
    "#fitting the model on the training set\n",
    "gbt = gbt.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on the test set\n",
    "prediction_gbt = gbt.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.90895460126015...|\n",
      "|  0.0|       0.0|[0.89036652651390...|\n",
      "|  0.0|       0.0|[0.88028985428337...|\n",
      "|  0.0|       0.0|[0.84332791236144...|\n",
      "|  0.0|       0.0|[0.92538630896616...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#comparing the predictions with known values\n",
    "prediction_gbt.select('label','prediction','probability').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.84\n",
      "Test Error = 0.16\n"
     ]
    }
   ],
   "source": [
    "#evaluator method to get the accuracy\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_gbt = round(evaluator.evaluate(prediction_gbt),2)\n",
    "print(\"Accuracy = \",accuracy_gbt)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy_gbt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Visualizing the Accuracy of all the models <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#creating a bar graph\n",
    "\n",
    "y=[accuracy_dt,accuracy_logit,accuracy_rf,accuracy_gbt]\n",
    "x=['Decision Tree','Logistic Regression','Random Forest','Gradient Boosting Tree']\n",
    "\n",
    "\n",
    "bar_width = 0.5\n",
    "\n",
    "x_pos = np.arange(len(x))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(x_pos, y, bar_width, align='center', color='C0')\n",
    "plt.xticks(x_pos, x, rotation='vertical')\n",
    "plt.xlabel('Classifiers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparison of Accuracy')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "for pos, n in zip(range(4), y):\n",
    "    plt.annotate(str(round(n,4)), xy=(pos-0.08 , n+0.003))\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above bar chart, we can see that the accuracy generated by all the models are really good with Gradient Boosting and Random Forest leading with same accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is model evaluation. Here, we evaluate the model using a metric named Confusion Matrix which basically tells us about the number of mis-classifications done by the model. \n",
    "\n",
    "A confusion matrix consists of 4 values and it is represented in the form of a matrix. These four values represent the following:\n",
    "\n",
    "- A true positive test result is one that detects the condition when the condition is present. \n",
    "- A true negative test result is one that does not detect the condition when the condition is absent. \n",
    "- A false positive test result is one that detects the condition when the condition is absent. \n",
    "- A false negative test result is one that does not detect the condition when the condition is present. \n",
    "\n",
    "We also determine various other measures from the confusion matrix such as precision, recall and f1-score.\n",
    "\n",
    "- Precision - Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances. \n",
    "- Recall - Recall (also known as sensitivity) is the fraction of the total amount of relevant instances that were actually retrieved. \n",
    "- F1-Score - The F measure (F1 score or F score) is a measure of a test's accuracy and is defined as the weighted harmonic mean of the precision and recall of the test.\n",
    "\n",
    "The above statistics are calculated as given in the picture below.\n",
    "\n",
    "\n",
    "<img src=\"https://daviddalpiaz.github.io/r4sl/images/confusion.png\" >\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/U0hjG.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we generate the confusion matrix and various other measures such as Precision, Recall and f1-score of each created model as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Decision Tree Model Evaluation <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix of Decision Tree Model is as follows: \n",
      "\n",
      "[[32272.   979.]\n",
      " [ 6287.  3061.]]\n",
      "--------------\n",
      "Decision Tree Model Metrics\n",
      "\n",
      "Accuracy = 0.83\n",
      "Precision = 0.33\n",
      "Recall = 0.76\n",
      "F1 Score = 0.46\n"
     ]
    }
   ],
   "source": [
    "#extracting the true label and model predictions into an key-value pair RDD\n",
    "prediction_dt_rdd = prediction_dt.select(['label', 'prediction']).rdd.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "#instantiating a MulticlassMetrics object \n",
    "metrics = MulticlassMetrics(prediction_dt_rdd)\n",
    "\n",
    "# displaying the confusion Matrix\n",
    "print(\"The confusion matrix of Decision Tree Model is as follows: \\n\")\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "# Retrieving the statistics from confusion matrix\n",
    "confusion_matrix=metrics.confusionMatrix().toArray()\n",
    "accuracy_dt=(confusion_matrix[0][0]+confusion_matrix[1][1])/confusion_matrix.sum()\n",
    "recall_dt=(confusion_matrix[1][1])/(confusion_matrix[1][1]+confusion_matrix[0][1])\n",
    "precision_dt=(confusion_matrix[1][1])/(confusion_matrix[1][1]+confusion_matrix[1][0]) \n",
    "f1score_dt = (2*precision_dt*recall_dt)/(precision_dt+recall_dt)\n",
    "\n",
    "print(\"--------------\\nDecision Tree Model Metrics\\n\")\n",
    "print(\"Accuracy = %s\" % round(accuracy_dt,2))\n",
    "print(\"Precision = %s\" % round(precision_dt,2))\n",
    "print(\"Recall = %s\" % round(recall_dt,2))\n",
    "print(\"F1 Score = %s\" % round(f1score_dt,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Logistic Regression Model Evaluation <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix of Logistic Regression Model is as follows: \n",
      "\n",
      "[[30847.  2404.]\n",
      " [ 5396.  3952.]]\n",
      "--------------\n",
      "Logistic Regression Model Metrics\n",
      "\n",
      "Accuracy = 0.82\n",
      "Precision = 0.42\n",
      "Recall = 0.62\n",
      "F1 Score = 0.5\n"
     ]
    }
   ],
   "source": [
    "#extracting the true label and model predictions into an key-value pair RDD\n",
    "prediction_log_rdd = prediction_log.select(['label', 'prediction']).rdd.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "#instantiating a MulticlassMetrics object \n",
    "metrics = MulticlassMetrics(prediction_log_rdd)\n",
    "\n",
    "# displaying the confusion Matrix\n",
    "print(\"The confusion matrix of Logistic Regression Model is as follows: \\n\")\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "# Retrieving the statistics from confusion matrix\n",
    "confusion_matrix=metrics.confusionMatrix().toArray()\n",
    "accuracy_log=(confusion_matrix[0][0]+confusion_matrix[1][1])/confusion_matrix.sum()\n",
    "recall_log=(confusion_matrix[1][1])/(confusion_matrix[1][1]+confusion_matrix[0][1])\n",
    "precision_log=(confusion_matrix[1][1])/(confusion_matrix[1][1]+confusion_matrix[1][0]) \n",
    "f1score_log = (2*precision_log*recall_log)/(precision_log+recall_log)\n",
    "\n",
    "print(\"--------------\\nLogistic Regression Model Metrics\\n\")\n",
    "print(\"Accuracy = %s\" % round(accuracy_log,2))\n",
    "print(\"Precision = %s\" % round(precision_log,2))\n",
    "print(\"Recall = %s\" % round(recall_log,2))\n",
    "print(\"F1 Score = %s\" % round(f1score_log,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Random Forest Model Evaluation <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix of Random Forest Model is as follows: \n",
      "\n",
      "[[31950.  1301.]\n",
      " [ 5737.  3611.]]\n",
      "--------------\n",
      "Random Forest Model Metrics\n",
      "\n",
      "Accuracy = 0.83\n",
      "Precision = 0.39\n",
      "Recall = 0.74\n",
      "F1 Score = 0.51\n"
     ]
    }
   ],
   "source": [
    "#extracting the true label and model predictions into an key-value pair RDD\n",
    "prediction_rf_rdd = prediction_rf.select(['label', 'prediction']).rdd.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "#instantiating a MulticlassMetrics object \n",
    "metrics = MulticlassMetrics(prediction_rf_rdd)\n",
    "\n",
    "# displaying the confusion Matrix\n",
    "print(\"The confusion matrix of Random Forest Model is as follows: \\n\")\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "# Retrieving the statistics from confusion matrix\n",
    "confusion_matrix=metrics.confusionMatrix().toArray()\n",
    "accuracy_rf=(confusion_matrix[0][0]+confusion_matrix[1][1])/confusion_matrix.sum()\n",
    "recall_rf=(confusion_matrix[1][1])/(confusion_matrix[1][1]+confusion_matrix[0][1])\n",
    "precision_rf=(confusion_matrix[1][1])/(confusion_matrix[1][1]+confusion_matrix[1][0]) \n",
    "f1score_rf = (2*precision_rf*recall_rf)/(precision_rf+recall_rf)\n",
    "\n",
    "print(\"--------------\\nRandom Forest Model Metrics\\n\")\n",
    "print(\"Accuracy = %s\" % round(accuracy_rf,2))\n",
    "print(\"Precision = %s\" % round(precision_rf,2))\n",
    "print(\"Recall = %s\" % round(recall_rf,2))\n",
    "print(\"F1 Score = %s\" % round(f1score_rf,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Gradient Boosting Model Evaluation <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix of Gradient Boosting Model is as follows: \n",
      "\n",
      "[[31612.  1639.]\n",
      " [ 5074.  4274.]]\n",
      "--------------\n",
      "Gradient Boosting Model Metrics\n",
      "\n",
      "Accuracy = 0.84\n",
      "Precision = 0.46\n",
      "Recall = 0.72\n",
      "F1 Score = 0.56\n"
     ]
    }
   ],
   "source": [
    "#extracting the true label and model predictions into an key-value pair RDD\n",
    "prediction_gbt_rdd = prediction_gbt.select(['label', 'prediction']).rdd.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "#instantiating a MulticlassMetrics object \n",
    "metrics = MulticlassMetrics(prediction_gbt_rdd)\n",
    "\n",
    "# displaying the confusion Matrix\n",
    "print(\"The confusion matrix of Gradient Boosting Model is as follows: \\n\")\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "# Retrieving the statistics from confusion matrix\n",
    "confusion_matrix=metrics.confusionMatrix().toArray()\n",
    "accuracy_gbt=(confusion_matrix[0][0]+confusion_matrix[1][1])/confusion_matrix.sum()\n",
    "recall_gbt=(confusion_matrix[1][1])/(confusion_matrix[1][1]+confusion_matrix[0][1])\n",
    "precision_gbt=(confusion_matrix[1][1])/(confusion_matrix[1][1]+confusion_matrix[1][0]) \n",
    "f1score_gbt = (2*precision_gbt*recall_gbt)/(precision_gbt+recall_gbt)\n",
    "\n",
    "print(\"--------------\\nGradient Boosting Model Metrics\\n\")\n",
    "print(\"Accuracy = %s\" % round(accuracy_gbt,2))\n",
    "print(\"Precision = %s\" % round(precision_gbt,2))\n",
    "print(\"Recall = %s\" % round(recall_gbt,2))\n",
    "print(\"F1 Score = %s\" % round(f1score_gbt,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise, the number of mis-classifications done by Gradient Boosting and Random Forest is less compared to Logistic Regression and Decision trees, thereby generating high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Strategies for Improving the Accuracy <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the strategies for improving the accuracy of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <u> Add More Data </u>: Having more data is always a good idea. It allows the data to tell for itself instead of relying on assumptions and weak correlations. Presence of more data results in better and accurate models.<br><br>\n",
    "\n",
    "* <u> More Significant Wrangling such as Outlier Detection </u>: Clean data helps to achieve the trade-off between bias and variance which improves the accuracy. Presence of outliers reduces the accuracy of the model and leads to a biased model. It leads to inaccurate predictions. This is because we dont analyse the behavior and relationship with other variables correctly. So, it is important to treat the outlier values well.<br><br>\n",
    "\n",
    "* <u> Feature Engineering </u>: Feature engineering such as feature scaling (changing the scale of variable from original scale to a scale between 0 and 1) leads to a normally distributed data. Machine learning algorithms work well with normally distributed data having a Gaussian Distribution. Also, deriving new variables from existing variables known as feature creation helps to unleash the hidden relationship of a data set.<br><br>\n",
    "\n",
    "* <u> Feature Selection </u>: Selecting relevant/important features that explains the target/response variable is the most important one. It is better to remove irrelevant features that do not have any association with the target. Various strategies can be used for feature selection such as removing features having low variance, correlation analysis or getting an idea of the domain knowledge through domain experts and then removing the features that are less important ones.<br><br>\n",
    "\n",
    "* <u> Fine Tuning of Parameters </u>: Fine tuning of parameters helps us to find the optimum value of each parameter which is used to boost the accuracy of the model. For example: In random forest, we have various parameters like max_features, number_trees, random_state, oob_score and others which is shown below:\n",
    "\n",
    "<b><i> RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None,min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None,bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False,class_weight=None) </i></b>\n",
    "\n",
    "* <u> Use of Ensemble Methods </u>: Using Ensembling methods such as Bagging (Bootstrap Aggregation) and Boosting helps to produce better results by combining the result of multiple weak models, thereby increasing the accuracy of the model.\n",
    "\n",
    "\n",
    "And finally,\n",
    "\n",
    "* <u> Cross Validation </u>: Using Cross Validation methods, like K-fold cross validaion and Leave One Out Cross Validationb (LOOCV), we can train the data across several different samples covering the entire subset of training data thereby providing the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Through this assignment, we have learnt how to use Machine Learning concepts using Apache Spark. We saw the performance of 4 different machine learning models on the weather data which produced accurate results. At the end, we discussed different strategies of improving the accuracy of the machine learning models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
